{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab:  Nonlinear Least Squares for Modeling Materials\n",
    "\n",
    "Nonlinear least squares (NLLS) is a widely-used method for modeling data.  In NLLS, we wish to fit a model of the form,\n",
    "\n",
    "     yhat = g(x,w)\n",
    "     \n",
    "where `w` is a vector of paramters and `x` is the vector of predictors.  We find `w` by minimizing a least-squares function \n",
    "\n",
    "     f(w) = \\sum_i (y_i - g(x_i,w))^2\n",
    "     \n",
    "where the summation is over training samples `(x_i,y_i)`.  This is similar to linear least-squares, but the function `g(x,w)` may not be linear in `w`.  In general, this optimization has no closed-form expression.  So numerical optimization must be used.  \n",
    "\n",
    "In this lab, we will implement gradient descent on NLLS in a problem of physical modeling of materials.  Specifically, we will estimate parameters for expansion of copper as a function of temperature using a real dataset.  In doing this lab, you will learn to:\n",
    "* Set up a nonlinear least squares as an unconstrained optimization function\n",
    "* Compute initial parameter estimates for a simple rational model\n",
    "* Compute the gradients of the least squares objective\n",
    "* Implement gradient descent for minimizing the objective\n",
    "* Implement momentum gradient descent\n",
    "* Visualize the convergence of the algorithm\n",
    "\n",
    "We first import some key packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "The NIST agency has an excellent [nonlinear regression website](https://www.itl.nist.gov/div898/strd/nls/nls_main.shtml) that has several datasets for nonlinear regression problems.  In this lab, we will use the data from a NIST study involving the thermal expansion of copper. The response variable is the coefficient of thermal expansion, and the predictor variable is temperature in degrees kelvin.  \n",
    "\n",
    "> Hahn, T., NIST (1979), Copper Thermal Expansion Study.  (unpublished}\n",
    "\n",
    "You can download the data as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.591</td>\n",
       "      <td>24.41</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.547</td>\n",
       "      <td>34.82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.902</td>\n",
       "      <td>44.09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.894</td>\n",
       "      <td>45.07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.703</td>\n",
       "      <td>54.98</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x0     y0  dummy\n",
       "0  0.591  24.41    NaN\n",
       "1  1.547  34.82    NaN\n",
       "2  2.902  44.09    NaN\n",
       "3  2.894  45.07    NaN\n",
       "4  4.703  54.98    NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://itl.nist.gov/div898/strd/nls/data/LINKS/DATA/Hahn1.dat'\n",
    "df = pd.read_csv(url, skiprows=60, sep=' ',skipinitialspace=True, names=['x0','y0','dummy'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the `x0` and `y0` into arrays.  Rescale, `x0` and `y0` to values between `0` and `1` by dividing `x0` and `y0` by the maximum value.  Store the scaled values in vectors `x` and `y`.  The rescaling will help with the conditioning of the fitting.  Plot, `y` vs. `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# x0 = ...\n",
    "# y0 = ...\n",
    "# x = x0/np.max(x0)\n",
    "# y = y0/np.max(y0)\n",
    "# plt.plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the problem a little more challenging, we will add some noise.  Add random Gaussian noise with mean 0 and std. dev = 0.05 to `y`.  Store the noisy results in `yn`. You can use the `np.random.normal()` function to add Gaussian noise. Plot `yn` vs. `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# yn = y + ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data `(x,yn)` into training and test.  Let `xtr,ytr` be training data and `xts,yts` be the test data.  You can use the `train_test_split` function.  Set `test_size=0.33` so that 1/3 of the samples are held out for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO\n",
    "# xtr, xts, ytr, yts = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Fit for a Rational Model\n",
    "\n",
    "The [NIST website](https://www.itl.nist.gov/div898/strd/nls/data/hahn1.shtml) suggests using a *rational* model of the form,\n",
    "\n",
    "      yhat = (a[0] + a[1]*x + ... + a[d]*x^d)/(1 + b[0]*x + ... + b[d-1]*x^d)\n",
    "      \n",
    "with `d=3`.  The model parameters are `w = [a[0],...,a[d],b[0],...,b[d-1]]` so there are `2d+1` parameters total.    Complete the function below that takes vectors `w` and `x` and predicts a set of values `yhat` using the above model.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,x):\n",
    "    \n",
    "    # Get the length\n",
    "    d = (len(w)-1)//2\n",
    "    \n",
    "    # TODO.  Extract a and b from w\n",
    "    # a = ...\n",
    "    # b = ...\n",
    "    \n",
    "    # TODO.  Compute yhat.  You may use the np.polyval function\n",
    "    # But, remember you must flip the order the a and b\n",
    "    # yhat = ...\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit with a nonlinear model, most methods only get convergence to a local minima.  So, you need a good initial condition.  For a rational model, one way to get is to realize that if:\n",
    "\n",
    "\n",
    "    y ~= (a[0] + a[1]*x + ... + a[d]*x^d)/(1 + b[0]*x + ... + b[d-1]*x^d)\n",
    "    \n",
    "Then:\n",
    "\n",
    "    y ~= a[0] + a[1]*x + ... + a[d]*x^d - b[0]*x*y + ... - b[d-1]*x^d*y.\n",
    "    \n",
    "So, we can solve for the the parameters `w = [a,b]` from linear regression of the predictors,\n",
    "\n",
    "    Z[i,:] = [ x[i], ... , x[i]**d, y[i]*x[i], ... , y[i}*x[i]**d ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3\n",
    "\n",
    "# TODO.  Create the transformed feature matrix\n",
    "# Z = ...\n",
    "\n",
    "# TODO.  Fit with parameters with linear regression\n",
    "# regr = LinearRegression()\n",
    "# regr.fit(...)\n",
    "\n",
    "# TODO\n",
    "# Extract the parameters from regr.coef_ and regr.intercept_ and store the parameter vector in winit\n",
    "# winit = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the predicted values of the `yhat` vs. `x` using your estimated parameter `winit` for 1000 values `x` in `[0,1]`.  On the same plot, plot `yts` vs. `xts`.  You will see that you get a horrible fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# xp = ...\n",
    "# yhat = ...\n",
    "# plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason the previous fit is poor is that the denominator in `yhat` goes close to zero.   To avoid this problem, we can use Ridge regression, to try to keep the parameters close to zero.  Re-run the fit above with `Ridge` with `alpha = 1e-3`. You should see you get a reasonable, but not perfect fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO.  Fit with parameters with linear regression\n",
    "# regr = Ridge(alpha=1e-3)\n",
    "# regr.fit(...)\n",
    "\n",
    "# TODO\n",
    "# Extract the parameters from regr.coef_ and regr.intercept_\n",
    "# winit = ...\n",
    "\n",
    "# TODO\n",
    "# Plot the results as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Loss Function\n",
    "\n",
    "We can now use gradient descent to improve our initial estimate.  Complete the following function to compute\n",
    "\n",
    "    f(w) = 0.5*\\sum_i (y[i] - yhat[i])^2\n",
    "    \n",
    "and `fgrad`, the gradient of `f(w)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feval(w,x,y):\n",
    "        \n",
    "    \n",
    "    # TODO.  Parse w\n",
    "    # a = ...\n",
    "    # b = ...\n",
    "    \n",
    "    # TODO.  Znum[i,j] = x[i]**j\n",
    "\n",
    "    # TODO.  Zden[i,j] = x[i]**(j+1)\n",
    "    \n",
    "    # TODO.  Compute yhat \n",
    "    # Compute the numerator and denominator\n",
    "    \n",
    "    # TODO.  Compute loss\n",
    "    # f = ...\n",
    "    \n",
    "    # TODO.  Compute gradients\n",
    "    # fgrad = ...\n",
    "    \n",
    "    return f, fgrad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the gradient function:\n",
    "* Take `w0=winit` and compute `f0,fgrad0 = feval(w0,xtr,ytr)`\n",
    "* Take `w1` very close to `w0` and compute `f1,fgrad1 = feval(w1,xtr,ytr)`\n",
    "* Verify that `f1-f0` is close to the predicted value based on the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement gradient descent\n",
    "\n",
    "We will now try to minimize the loss function with gradient descent.  Using the function `feval` defined above, implement gradient descent.  Run gradient descent with a step size of `alpha=1e-6` starting at `w=winit`.  Run it for `nit=10000` iterations.  Compute `fgd[it]`= the objective function on iteration `it`.  Plot `fgd[it]` vs. `it`.  \n",
    "\n",
    "You should see that the training loss decreases, but it still hasn't converged after 10000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# fgd = ...\n",
    "nit = 10000\n",
    "step = 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try to get a faster convergence with adaptive step-size using the Armijo rule. Implement the gradient descent with adaptive step size.  Let `fadapt[it]` be the loss function on iteration `it`.  Plot `fadapt[it]` and `fgd[it]` vs. `it` on the same graph.  You should see a slight improvement, but not much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# fadapt = ...\n",
    "nit = 10000\n",
    "step = 1e-6  # Initial step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using he final estimate for `w` from the adaptive step-size plot the predicted values of the `yhat` vs. `x` usfor 1000 values `x` in `[0,1]`.  On the same plot, plot `yhat` vs. `x` for the initial parameter `w=winit`.  Also, plot `yts` vs. `xts`.  You should see that gradient descent was able to improve the estimat slightly, although the initial estimate was not too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# xp = np.linspace(...)\n",
    "# yhat = ...\n",
    "# plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Gradient Descent\n",
    "\n",
    "This section is bonus.\n",
    "\n",
    "One way to improve gradient descent is to use *momentum*.  In momentum gradient descent, the update rule is:\n",
    "\n",
    "    f, fgrad = feval(w,...)\n",
    "    z = beta*z + fgrad\n",
    "    w = w - step*z\n",
    "    \n",
    "This is similar to gradient descent, except that there is a second order term on the gradient.  Implement this algorithm with `beta = 0.99` and `step=1e-5`.   Compare the convergence of the loss function with gradient descent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "nit = 1000\n",
    "step = 1e-5\n",
    "beta = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# plot yhat vs. x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond This Lab\n",
    "In this lab, we have just touched at some of the ideas in optimization.  There are several other important algorithms that you can explore:\n",
    "* [Levenberg-Marquardt](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) method for non-linear least squares\n",
    "* Newton's method\n",
    "* More difficult non-linear least squares problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
